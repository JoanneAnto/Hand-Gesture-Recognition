# Hand-Gesture-Recognition

Human gestures are undoubtedly natural. They can often prove to be more effective and powerful compared to various other ways of interaction. Gesture recognition involves tracking the human position, orientation, or movement and finally interpreting them to recognize semantically consequent gestures. Tracking head/arm or body position or configuration can be precious for controlling objects/systems or supplying input parameters to a system. The range of applications includes clinical health,
robotic control, military simulation, et. Gloves, and sensor-based trackers are cumbersome, limited, and challenging to use. Using the hand directly as an input device
is an innovative way to provide natural human-computer interaction, ranging from text-based interfaces to 2D graphical interfaces, to multimedia-enabled interfaces, to full-fledged multiple There is a legacy up to the participant virtual environment system. Image-based techniques detect a gesture by capturing images of the userâ€™s
movements during the gesture. The system sends these images to computer vision software, which tracks them and identifies the gesture. Many previously developed models were available and could only be identified from a static background. Since sign language is gestured fluently and interactively like any other spoken language, sign language recognizers must be able to recognize continuous sign language vocabulary in real-time.

The purpose of this project is to present a real-time hand gesture recognition system based on American Sign Language (ASL) recognition in natural environments. It does not require uniform clothing and can work reliably in a messy, non-static background. The system captures gestural images of ASL from a cluttered background using a webcam. A pre-trained model, SqueezeNet, is used for image classification, and using this model, hand gestures of alphabets and letters are identified and displayed in a real-time interface.
